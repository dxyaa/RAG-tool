{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain_community chromadb langchain_google_genai bs4 numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMMathChain\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool, DuckDuckGoSearchRun\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import AgentExecutor\n",
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "#enter your gemini api key\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"API Key - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                 temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "class ConversationBuffer:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def add_to_history(self, question, answer):\n",
    "        self.history.append((question, answer))\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "    def clear_history(self):\n",
    "        self.history.clear()\n",
    "        return self.history\n",
    "\n",
    "def get_chat_history():\n",
    "    history = conversation_buffer.get_history()\n",
    "    #print(\"check\")\n",
    "    formatted_history = {\"history\": history}\n",
    "    return formatted_history\n",
    "conversation_buffer = ConversationBuffer()\n",
    "print(conversation_buffer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_with_history(question):\n",
    "    result = rag_chain.invoke(question)\n",
    "    conversation_buffer.history[-1] = (question, result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Take the question and find relevant information from the given context and return the generated answer.\n",
    "Use the following context to answer the question.\n",
    "Use chat history to get more context whenever necessary.\n",
    "If you don't know the answer or if the answer is irrelevent, just say that you don't know as response.\n",
    "Use five sentences maximum and keep the answer friendly.\\n\n",
    "Question: {question} \\nContext: {context}\\nChat History: {chatHistory} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#give doc website link below or documents using doc loaders.\n",
    "loader = WebBaseLoader(\"\")\n",
    "docs = loader.load()\n",
    "text_content = docs[0].page_content\n",
    "text_content_1 = text_content.split(\"code, audio, image and video.\",1)[0]\n",
    "final_text = text_content_1.split(\"Cloud TPU v5p\",1)[0]\n",
    "docs =  [Document(page_content=final_text, metadata={\"source\": \"local\"})]\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "                     documents=docs,\n",
    "                     embedding=gemini_embeddings,\n",
    "                     persist_directory=\"./chroma_db\"\n",
    "                     )\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",\n",
    "                        embedding_function=gemini_embeddings\n",
    "                   )\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})\n",
    "print(len(retriever.get_relevant_documents(\"nissan\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs , \"question\": RunnablePassthrough(),\"chatHistory\":lambda x:conversation_buffer.get_history()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm_math\\base.py:58: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "#Enter the document topic in the tool description.\n",
    "search_tool = Tool.from_function(\n",
    "    func=rag_chain.invoke,\n",
    "    name=\"SearchDoc\",\n",
    "    description=\"Useful when u need to find answers to the question related to the topic -  . pass user's input as is without any changes\"\n",
    ")\n",
    "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
    "math_tool = Tool.from_function(\n",
    "    func=llm_math_chain.run,\n",
    "    name=\"Calculator\",\n",
    "    description=\"Useful for when you are asked to perform math calculations\"\n",
    ")\n",
    "tools = [search_tool, math_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors = True\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Calculator to perform the addition\n",
      "Action: Calculator\n",
      "Action Input: 2+2\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "2+2\u001b[32;1m\u001b[1;3m```text\n",
      "2+2\n",
      "```\n",
      "...numexpr.evaluate(\"2+2\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m4\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "question=\"What is two plus two?\"\n",
    "conversation_buffer.add_to_history(question, None)\n",
    "x=agent.invoke(question)[\"output\"]\n",
    "#print(llm.usage_metadeta)\n",
    "conversation_buffer.history[-1] = (question,x )\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What is two plus two?', '4')]\n"
     ]
    }
   ],
   "source": [
    "#to get full history of chat\n",
    "print(conversation_buffer.get_history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to clear full history\n",
    "print(conversation_buffer.clear_history())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
